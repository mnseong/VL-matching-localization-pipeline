{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "import skimage.io as io\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import importlib\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [],
   "source": [
    "if not torch.backends.mps.is_available():\n",
    "    if not torch.backends.mps.is_built():\n",
    "        print(\"MPS not available because the current PyTorch install was not \"\n",
    "              \"built with MPS enabled.\")\n",
    "    else:\n",
    "        print(\"MPS not available because the current MacOS version is not 12.3+ \"\n",
    "              \"and/or you do not have an MPS-enabled device on this machine.\")\n",
    "\n",
    "else:\n",
    "    mps_device = torch.device(\"mps\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [
    {
     "data": {
      "text/plain": "device(type='mps')"
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mps_device"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=(0.485, 0.456, 0.406),\n",
    "                                                     std=(0.229, 0.224, 0.225)),\n",
    "                                ])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([3, 1056, 1600])"
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdir = Path('test/')\n",
    "dbimgs = imdir.glob('*.jpg')\n",
    "imf = dbimgs[0]\n",
    "im = io.imread(imf)\n",
    "im_tensor = transform(im)\n",
    "c, h, w = im_tensor.shape\n",
    "sift = cv2.xfeatures2d.SIFT_create()\n",
    "\n",
    "pad=(0,0,0,0)\n",
    "\n",
    "crop_r = w%16\n",
    "crop_b = h%16\n",
    "im_tensor = im_tensor[:,:h-crop_b,:w-crop_r]\n",
    "im = im[:h-crop_b,:w-crop_r,:]\n",
    "gray = cv2.cvtColor(im, cv2.COLOR_RGB2GRAY)\n",
    "kpts = sift.detect(gray)\n",
    "kpts = np.array([[kp.pt[0], kp.pt[1]] for kp in kpts])\n",
    "coord = torch.from_numpy(kpts).float()\n",
    "im1_ori = torch.from_numpy(im)\n",
    "out = {'im1': im_tensor, 'im1_ori':im1_ori, 'coord1': coord, 'pad1':pad}\n",
    "out['im1'].shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 3, 1056, 1600])"
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sample = out['im1']\n",
    "b = 1\n",
    "input_4d = torch.unsqueeze(input_sample, 0).expand(b, -1, -1, -1)\n",
    "input_4d.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [],
   "source": [
    "def class_for_name(module_name, class_name):\n",
    "    # load the module, will raise ImportError if module cannot be loaded\n",
    "    m = importlib.import_module(module_name)\n",
    "    return getattr(m, class_name)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [],
   "source": [
    "class conv(nn.Module):\n",
    "    def __init__(self, num_in_layers, num_out_layers, kernel_size, stride):\n",
    "        super(conv, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.conv = nn.Conv2d(num_in_layers,\n",
    "                              num_out_layers,\n",
    "                              kernel_size=kernel_size,\n",
    "                              stride=stride,\n",
    "                              padding=(self.kernel_size - 1) // 2)\n",
    "        self.gn = nn.GroupNorm(num_groups=32,num_channels=num_out_layers) # Group Norm with 32\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.elu(self.gn(self.conv(x)), inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [],
   "source": [
    "class upconv(nn.Module):\n",
    "    def __init__(self, num_in_layers, num_out_layers, kernel_size, scale):\n",
    "        super(upconv, self).__init__()\n",
    "        self.scale  = scale\n",
    "        self.conv   = conv(num_in_layers, num_out_layers, kernel_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.functional.interpolate(x, scale_factor=self.scale, align_corners=True, mode='bilinear')\n",
    "        return self.conv(x)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [],
   "source": [
    "class upconv_like(nn.Module):\n",
    "    def __init__(self, num_in_layers, num_out_layers, kernel_size):\n",
    "        super(upconv_like, self).__init__()\n",
    "\n",
    "        self.conv = conv(num_in_layers, num_out_layers, kernel_size, 1)\n",
    "\n",
    "    def forward(self, x, target):\n",
    "        x = F.upsample(x,size=target.shape[2:],mode='bilinear')\n",
    "        return self.conv(x)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [],
   "source": [
    "class Squeeze_Excite_Block(nn.Module):\n",
    "    def __init__(self, channel, reduction=16):\n",
    "        super(Squeeze_Excite_Block, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channel, channel // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channel // reduction, channel, bias=False),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        return x * y.expand_as(x)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [],
   "source": [
    "class PositionwiseNorm2(nn.Module):\n",
    "    def __init__(self, epsilon=1e-5):\n",
    "        super().__init__()\n",
    "        self.epsilon    = epsilon\n",
    "        self.conv1      = nn.Conv2d(1,1,kernel_size=7,stride=1,padding=3)\n",
    "        self.conv2      = nn.Conv2d(1, 1, kernel_size=7, stride=1, padding=3)\n",
    "    def forward(self,x):\n",
    "        mean = x.mean(dim=1, keepdim=True)\n",
    "        std = x.var(dim=1, keepdim=True).add(self.epsilon).sqrt()\n",
    "        output = (x - mean) / std\n",
    "        map = torch.mean(x,dim=1, keepdim=True)\n",
    "        map1 = self.conv1(map)\n",
    "        map2 = self.conv2(map)\n",
    "        return output*map1 + map2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [],
   "source": [
    "class Adaffusion(nn.Module):\n",
    "    def __init__(self, num_channels):\n",
    "        super().__init__()\n",
    "        self.num_channels   = num_channels\n",
    "        self.avg_pool       = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc1            = nn.Linear(128,64)\n",
    "        self.relu1          = nn.ReLU()\n",
    "        self.fc2            = nn.Linear(64,128)\n",
    "        self.fc3            = nn.Linear(128, 64)\n",
    "        self.relu2          = nn.ReLU()\n",
    "        self.fc4            = nn.Linear(64, 128)\n",
    "\n",
    "    def forward(self, result, x):\n",
    "        avg_out1 = self.fc2(self.relu1(self.fc1(self.avg_pool(x).squeeze(-1).squeeze(-1)))).unsqueeze(-1).unsqueeze(-1)\n",
    "        avg_out2 = self.fc4(self.relu2(self.fc3(self.avg_pool(x).squeeze(-1).squeeze(-1)))).unsqueeze(-1).unsqueeze(-1)\n",
    "        return result * avg_out1 + avg_out2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [],
   "source": [
    "class ChannelwiseNorm(nn.Module):\n",
    "    def __init__(self, num_features, momentum=0.9, eps=1e-5, affusion=False, track_running_stats=False):\n",
    "        super().__init__()\n",
    "        self.momentum = momentum\n",
    "        self.eps = eps\n",
    "\n",
    "        if affusion:\n",
    "            self.affusion = Adaffusion(num_features)\n",
    "        else:\n",
    "            self.affusion = None\n",
    "\n",
    "        self.track_running_stats = track_running_stats\n",
    "        if track_running_stats:\n",
    "            self.register_buffer('running_mean', torch.zeros(num_features))\n",
    "            self.register_buffer('running_var', torch.ones(num_features))\n",
    "        else:\n",
    "            self.register_parameter('running_mean', None)\n",
    "            self.register_parameter('running_var', None)\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert len(x.shape) == 4\n",
    "        b, c, h, w = x.shape\n",
    "\n",
    "        if self.training or not self.track_running_stats:\n",
    "            # All dims except for B and C\n",
    "            mu = x.mean(dim=(2, 3))\n",
    "            sigma = x.var(dim=(2, 3), unbiased=False)\n",
    "        else:\n",
    "            mu, sigma = self.running_mean, self.running_var\n",
    "            b = 1\n",
    "\n",
    "        if self.training and self.track_running_stats:\n",
    "            sigma_unbiased = sigma * ((h * w) / ((h * w) - 1))\n",
    "            self.running_mean   = self.running_mean * (1 - self.momentum) + mu.mean(dim=0) * self.momentum\n",
    "            self.running_var    = self.running_var * (1 - self.momentum) + sigma_unbiased.mean(dim=0) * self.momentum\n",
    "\n",
    "        mu = mu.reshape(b, c, 1, 1)\n",
    "        sigma = sigma.reshape(b, c, 1, 1)\n",
    "        result = (x - mu) / torch.sqrt(sigma + self.eps)\n",
    "\n",
    "        if self.affusion is not None:\n",
    "            result = self.affusion(result)\n",
    "\n",
    "        return result"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([1, 128, 264, 400])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_channels, reduction_ratio=16):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "        self.fc1 = nn.Conv2d(in_channels, in_channels // reduction_ratio, kernel_size=1, stride=1, padding=0)\n",
    "        self.fc2 = nn.Conv2d(in_channels // reduction_ratio, in_channels, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = self.fc2(self.fc1(self.avg_pool(x)))\n",
    "        max_out = self.fc2(self.fc1(self.max_pool(x)))\n",
    "        out = avg_out + max_out\n",
    "        return torch.sigmoid(out)\n",
    "\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "        self.conv = nn.Conv2d(2, 1, kernel_size=7, stride=1, padding=3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        out = torch.cat([avg_out, max_out], dim=1)\n",
    "        out = self.conv(out)\n",
    "        return torch.sigmoid(out)\n",
    "\n",
    "\n",
    "class CBAM(nn.Module):\n",
    "    def __init__(self, in_channels, reduction_ratio=16):\n",
    "        super(CBAM, self).__init__()\n",
    "        self.channel_attention = ChannelAttention(in_channels, reduction_ratio)\n",
    "        self.spatial_attention = SpatialAttention()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.channel_attention(x) * x\n",
    "        out = self.spatial_attention(out) * out\n",
    "        return out\n",
    "\n",
    "# Test the implementation\n",
    "if __name__ == \"__main__\":\n",
    "    # Assuming input channel is 64\n",
    "    input_tensor = torch.randn(1, 128, 264, 400)\n",
    "    cbam_module = CBAM(in_channels=128)\n",
    "    output = cbam_module(input_tensor)\n",
    "    print(\"Output shape:\", output.shape)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "outputs": [],
   "source": [
    "class MinseongNet(nn.Module):\n",
    "    def __init__(self,\n",
    "                 encoder='resnet50',\n",
    "                 pretrained=True,\n",
    "                 fusion_out_ch=128\n",
    "                 ):\n",
    "\n",
    "        super(MinseongNet, self).__init__()\n",
    "        assert encoder in ['resnet18', 'resnet34', 'resnet50', 'resnet101', 'resnet152'], \"Incorrect encoder type\"\n",
    "        if encoder in ['resnet18', 'resnet34']:\n",
    "            filters = [64, 128, 256, 512]\n",
    "        else:\n",
    "            filters = [256, 512, 1024, 2048]\n",
    "        resnet = class_for_name(\"torchvision.models\", encoder)(pretrained=pretrained)\n",
    "        resnet_152 = class_for_name(\"torchvision.models\", 'resnet152')(pretrained=pretrained)\n",
    "\n",
    "        self.firstconv      = resnet.conv1  # H/2\n",
    "        self.firstbn        = resnet.bn1\n",
    "        self.firstrelu      = resnet.relu\n",
    "        self.firstmaxpool   = resnet.maxpool  # H/4\n",
    "\n",
    "        # Encoder\n",
    "        self.layer1 = resnet.layer1  # H/4\n",
    "        self.layer2 = resnet.layer2  # H/8\n",
    "        self.layer3 = resnet.layer3  # H/16\n",
    "\n",
    "        # Decoder\n",
    "        self.upconv3    = upconv(filters[2], 512, 3, 2)\n",
    "        self.iconv3     = conv(filters[1] + 512, 512, 3, 1)\n",
    "        self.upconv2    = upconv(512, 256, 3, 2)\n",
    "        self.iconv2     = conv(filters[0] + 256, 256, 3, 1)\n",
    "\n",
    "        # 152_Encoder\n",
    "        self.layer152_1 = resnet_152.layer1  # H/4\n",
    "        self.layer152_2 = resnet_152.layer2  # H/8\n",
    "        self.layer152_3 = resnet_152.layer3  # H/16\n",
    "\n",
    "        # 152_Decoder\n",
    "        self.upconv152_3    = upconv(filters[2], 512, 3, 2)\n",
    "        self.iconv152_3     = conv(filters[1] + 512, 512, 3, 1)\n",
    "        self.upconv152_2    = upconv(512, 256, 3, 2)\n",
    "        self.iconv152_2     = conv(filters[0] + 256, 256, 3, 1)\n",
    "\n",
    "        # Feature Fusion Block\n",
    "        self.side3          = upconv_like(1024, fusion_out_ch, 3)\n",
    "        self.side2          = upconv_like(512, fusion_out_ch, 3)\n",
    "        self.side1          = conv(256, fusion_out_ch, 1, 1)\n",
    "        self.fusion_conv    = nn.Conv2d(3*fusion_out_ch,fusion_out_ch,1)\n",
    "\n",
    "        # Cross Norm. Layer\n",
    "        self.fusion_pn = PositionwiseNorm2()\n",
    "        self.fusion_cn = ChannelwiseNorm(fusion_out_ch)\n",
    "        self.fuse_weight_fusion_1 = torch.nn.Parameter(torch.FloatTensor(1), requires_grad=True)\n",
    "        self.fuse_weight_fusion_2 = torch.nn.Parameter(torch.FloatTensor(1), requires_grad=True)\n",
    "        self.fuse_weight_fusion_1.data.fill_(0.7)\n",
    "        self.fuse_weight_fusion_2.data.fill_(0.3)\n",
    "\n",
    "        self.out_channels = 192\n",
    "\n",
    "        self.conv1 = nn.Conv2d(128, 128, 3, 1, 1)\n",
    "        self.conv2 = nn.Conv2d(256, 128, 3, 1, 1)\n",
    "        self.conv3 = nn.Conv2d(128, 1, 1, 1, 0)\n",
    "\n",
    "        self.norm1 = nn.InstanceNorm2d(128)\n",
    "        self.norm2 = nn.InstanceNorm2d(128)\n",
    "        self.norm3 = nn.InstanceNorm2d(1)\n",
    "        self.relu  = nn.PReLU()\n",
    "\n",
    "        self.cbam = CBAM(in_channels=128)\n",
    "\n",
    "    def name(self):\n",
    "        return 'MinseongNet'\n",
    "\n",
    "    def skipconnect(self, x1, x2):\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "\n",
    "        x1 = F.pad(x1, (diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2))\n",
    "\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return x\n",
    "\n",
    "    def peakiness_score(self, x, ksize=3, dilation=1):\n",
    "        # [b, c, h, w] the feature maps\n",
    "        # [b, 1, h, w] the peakiness score map\n",
    "        b,c,h,w = x.shape\n",
    "        max_per_sample = torch.max(x.view(b,-1), dim=1)[0]\n",
    "        x = x / max_per_sample.view(b,1,1,1)\n",
    "\n",
    "        pad_inputs = F.pad(x, [dilation]*4, mode='reflect')\n",
    "        avg_inputs = F.avg_pool2d(pad_inputs, ksize, stride=1)\n",
    "\n",
    "        alpha   = F.softplus(x - avg_inputs)\n",
    "        beta    = F.softplus(x - x.mean(1, True))\n",
    "\n",
    "        score_vol = alpha * beta\n",
    "        score_map = score_vol.max(1,True)[0]\n",
    "\n",
    "        return score_map\n",
    "\n",
    "    def forward(self, x):\n",
    "        x       = self.firstrelu(self.firstbn(self.firstconv(x)))\n",
    "        x_first = self.firstmaxpool(x)\n",
    "\n",
    "        # First Encoder Pipe\n",
    "        x1 = self.layer1(x_first)\n",
    "        x2 = self.layer2(x1)\n",
    "        x3 = self.layer3(x2)\n",
    "\n",
    "        # Second Encoder Pipe\n",
    "        y1 = self.layer152_1(x_first)\n",
    "        y2 = self.layer152_2(y1)\n",
    "        y3 = self.layer152_3(y2)\n",
    "\n",
    "        print(\"이건 x3\")\n",
    "        print(x3.shape)\n",
    "        print(\"이건 y3\")\n",
    "        print(y3.shape)\n",
    "\n",
    "        # First Decoder Pipe\n",
    "        up_x3   = self.upconv152_3(x3)\n",
    "        y   = self.skipconnect(y2, up_x3)\n",
    "        y2d = self.iconv152_3(y)\n",
    "        up_y2d = self.upconv152_2(y2d)\n",
    "        x = self.skipconnect(x1, up_y2d)\n",
    "\n",
    "        a = self.iconv152_2(x)\n",
    "\n",
    "        # Second Decoder Pipe\n",
    "        up_y3   = self.upconv152_3(y3)\n",
    "        x   = self.skipconnect(x2, up_y3)\n",
    "        x2d = self.iconv152_3(x)\n",
    "        up_x2d = self.upconv152_2(x2d)\n",
    "        y = self.skipconnect(y1, up_x2d)\n",
    "\n",
    "        b = self.iconv152_2(y)\n",
    "\n",
    "        # Fit into proper form\n",
    "        a_d1          = self.side1(a)\n",
    "        b_d1          = self.side1(b)\n",
    "        a_d2          = self.side2(y2d, a_d1)\n",
    "        b_d2          = self.side2(x2d, b_d1)\n",
    "        a_d3          = self.side3(x3, a_d1)\n",
    "        b_d3          = self.side3(y3, b_d1)\n",
    "\n",
    "        # Feature Fusion output\n",
    "        a_fusion      = self.fusion_conv(torch.cat((a_d1,a_d2,a_d3),1))  # H/4\n",
    "        b_fusion      = self.fusion_conv(torch.cat((b_d1,b_d2,b_d3),1))  # H/4\n",
    "\n",
    "        # Shared Coupling-bridge Normalization\n",
    "        desc1       = self.fusion_pn(a_fusion)\n",
    "        desc2       = self.fusion_cn(a_fusion)\n",
    "        a_fusion_cn = desc1 * (self.fuse_weight_fusion_1/(self.fuse_weight_fusion_1+self.fuse_weight_fusion_2)) + \\\n",
    "                      desc2 * (self.fuse_weight_fusion_2/(self.fuse_weight_fusion_1+self.fuse_weight_fusion_2))\n",
    "\n",
    "        desc3       = self.fusion_pn(b_fusion)\n",
    "        desc4       = self.fusion_cn(b_fusion)\n",
    "        b_fusion_cn = desc3 * (self.fuse_weight_fusion_1/(self.fuse_weight_fusion_1+self.fuse_weight_fusion_2)) + \\\n",
    "                      desc4 * (self.fuse_weight_fusion_2/(self.fuse_weight_fusion_1+self.fuse_weight_fusion_2))\n",
    "\n",
    "        # Measure peakiness score a\n",
    "        a_pf = self.peakiness_score(a_fusion_cn)\n",
    "        a_mps = self.relu(self.norm1(self.conv1(a_pf*a_fusion_cn)))\n",
    "\n",
    "        # CBAM b\n",
    "        b_cbam = self.cbam(b_fusion_cn)\n",
    "\n",
    "        final = torch.cat([a_mps, b_cbam], dim=1)\n",
    "        final = self.relu(self.norm2(self.conv2(final)))\n",
    "        final = F.softplus(self.norm3(self.conv3(final)))\n",
    "\n",
    "        # out = {'a_fusion_cn': a_fusion_cn,\n",
    "        #        'b_fusion_cn': b_fusion_cn,\n",
    "        #        'final': final\n",
    "        #        }\n",
    "        return final"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kweonminseong/.pyenv/versions/minseongnet/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/kweonminseong/.pyenv/versions/minseongnet/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/Users/kweonminseong/.pyenv/versions/minseongnet/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet152_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet152_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "net = MinseongNet()\n",
    "#summary(net, (3, 224, 224))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이건 x3\n",
      "torch.Size([1, 1024, 66, 100])\n",
      "이건 y3\n",
      "torch.Size([1, 1024, 66, 100])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kweonminseong/.pyenv/versions/minseongnet/lib/python3.9/site-packages/torch/nn/functional.py:3737: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n"
     ]
    }
   ],
   "source": [
    "result = net(input_4d)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 1, 264, 400])"
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "outputs": [],
   "source": [
    "# 평균과 표준편차\n",
    "mean = (0.485, 0.456, 0.406)\n",
    "std = (0.229, 0.224, 0.225)\n",
    "\n",
    "# Normalize된 텐서를 다시 Denormalize하는 함수\n",
    "def denormalize(tensor):\n",
    "    # 텐서를 CPU로 이동하고 복사\n",
    "    tensor = tensor.cpu().clone()\n",
    "    # 텐서를 Denormalize (역변환)\n",
    "    for t, m, s in zip(tensor, mean, std):\n",
    "        t.mul_(s).add_(m)\n",
    "    return tensor"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "outputs": [],
   "source": [
    "# mean, std for ImageNet prt\n",
    "mean = (0.485, 0.456, 0.406)\n",
    "std = (0.229, 0.224, 0.225)\n",
    "\n",
    "# Denormalize function (pretrained by imagenet)\n",
    "def denormalize(tensor):\n",
    "    tensor = tensor.cpu().clone()\n",
    "    for t, m, s in zip(tensor, mean, std):\n",
    "        t.mul_(s).add_(m)\n",
    "    return tensor"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "minseongnet",
   "language": "python",
   "display_name": "minseongnet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
