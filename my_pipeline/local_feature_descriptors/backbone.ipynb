{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98c7217d-8e6d-4803-8d6c-4c213d53e37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e0d6d0f-5b07-453f-b583-63ebd26c8b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_for_name(module_name, class_name):\n",
    "    # load the module, will raise ImportError if module cannot be loaded\n",
    "    m = importlib.import_module(module_name)\n",
    "    return getattr(m, class_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d7dcdc5-eb3b-4ec4-b581-ebc05f6b8b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class conv(nn.Module):\n",
    "    def __init__(self, num_in_layers, num_out_layers, kernel_size, stride):\n",
    "        super(conv, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.conv = nn.Conv2d(num_in_layers,\n",
    "                              num_out_layers,\n",
    "                              kernel_size=kernel_size,\n",
    "                              stride=stride,\n",
    "                              padding=(self.kernel_size - 1) // 2)\n",
    "        self.gn = nn.GroupNorm(num_groups=32,num_channels=num_out_layers) # Group Norm with 32\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.elu(self.gn(self.conv(x)), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7f2b8e6-5db5-4874-afdc-80b385c747b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class upconv(nn.Module):\n",
    "    def __init__(self, num_in_layers, num_out_layers, kernel_size, scale):\n",
    "        super(upconv, self).__init__()\n",
    "        self.scale  = scale\n",
    "        self.conv   = conv(num_in_layers, num_out_layers, kernel_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.functional.interpolate(x, scale_factor=self.scale, align_corners=True, mode='bilinear')\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2133a9d-1bd2-4672-ae3a-02c652bbab68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class upconv_like(nn.Module):\n",
    "    def __init__(self, num_in_layers, num_out_layers, kernel_size):\n",
    "        super(upconv_like, self).__init__()\n",
    "\n",
    "        self.conv = conv(num_in_layers, num_out_layers, kernel_size, 1)\n",
    "\n",
    "    def forward(self, x, target):\n",
    "        x = F.upsample(x,size=target.shape[2:],mode='bilinear')\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66d78e6c-705e-4abb-8eab-db1ccab82ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Squeeze_Excite_Block(nn.Module):\n",
    "    def __init__(self, channel, reduction=16):\n",
    "        super(Squeeze_Excite_Block, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channel, channel // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channel // reduction, channel, bias=False),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        return x * y.expand_as(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "131f21ca-2824-4cbc-8375-7f7ba3f51c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseNorm2(nn.Module):\n",
    "    def __init__(self, epsilon=1e-5):\n",
    "        super().__init__()\n",
    "        self.epsilon    = epsilon\n",
    "        self.conv1      = nn.Conv2d(1,1,kernel_size=7,stride=1,padding=3)\n",
    "        self.conv2      = nn.Conv2d(1, 1, kernel_size=7, stride=1, padding=3)\n",
    "    def forward(self,x):\n",
    "        mean = x.mean(dim=1, keepdim=True)\n",
    "        std = x.var(dim=1, keepdim=True).add(self.epsilon).sqrt()\n",
    "        output = (x - mean) / std\n",
    "        map = torch.mean(x,dim=1, keepdim=True)\n",
    "        map1 = self.conv1(map)\n",
    "        map2 = self.conv2(map)\n",
    "        return output*map1 + map2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f45c1822-4387-4cff-a554-9ef0f1906a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adaffusion(nn.Module):\n",
    "    def __init__(self, num_channels):\n",
    "        super().__init__()\n",
    "        self.num_channels   = num_channels\n",
    "        self.avg_pool       = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc1            = nn.Linear(128,64)\n",
    "        self.relu1          = nn.ReLU()\n",
    "        self.fc2            = nn.Linear(64,128)\n",
    "        self.fc3            = nn.Linear(128, 64)\n",
    "        self.relu2          = nn.ReLU()\n",
    "        self.fc4            = nn.Linear(64, 128)\n",
    "\n",
    "    def forward(self, result, x):\n",
    "        avg_out1 = self.fc2(self.relu1(self.fc1(self.avg_pool(x).squeeze(-1).squeeze(-1)))).unsqueeze(-1).unsqueeze(-1)\n",
    "        avg_out2 = self.fc4(self.relu2(self.fc3(self.avg_pool(x).squeeze(-1).squeeze(-1)))).unsqueeze(-1).unsqueeze(-1)\n",
    "        return result * avg_out1 + avg_out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6958528-5f8e-43e3-b449-23b01d1c5af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChannelwiseNorm(nn.Module):\n",
    "    def __init__(self, num_features, momentum=0.9, eps=1e-5, affusion=False, track_running_stats=False):\n",
    "        super().__init__()\n",
    "        self.momentum = momentum\n",
    "        self.eps = eps\n",
    "\n",
    "        if affusion:\n",
    "            self.affusion = Adaffusion(num_features)\n",
    "        else:\n",
    "            self.affusion = None\n",
    "\n",
    "        self.track_running_stats = track_running_stats\n",
    "        if track_running_stats:\n",
    "            self.register_buffer('running_mean', torch.zeros(num_features))\n",
    "            self.register_buffer('running_var', torch.ones(num_features))\n",
    "        else:\n",
    "            self.register_parameter('running_mean', None)\n",
    "            self.register_parameter('running_var', None)\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert len(x.shape) == 4\n",
    "        b, c, h, w = x.shape\n",
    "\n",
    "        if self.training or not self.track_running_stats:\n",
    "            # All dims except for B and C\n",
    "            mu = x.mean(dim=(2, 3))\n",
    "            sigma = x.var(dim=(2, 3), unbiased=False)\n",
    "        else:\n",
    "            mu, sigma = self.running_mean, self.running_var\n",
    "            b = 1\n",
    "\n",
    "        if self.training and self.track_running_stats:\n",
    "            sigma_unbiased = sigma * ((h * w) / ((h * w) - 1))\n",
    "            self.running_mean   = self.running_mean * (1 - self.momentum) + mu.mean(dim=0) * self.momentum\n",
    "            self.running_var    = self.running_var * (1 - self.momentum) + sigma_unbiased.mean(dim=0) * self.momentum\n",
    "\n",
    "        mu = mu.reshape(b, c, 1, 1)\n",
    "        sigma = sigma.reshape(b, c, 1, 1)\n",
    "        result = (x - mu) / torch.sqrt(sigma + self.eps)\n",
    "\n",
    "        if self.affusion is not None:\n",
    "            result = self.affusion(result)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "871b303f-01c5-43cc-8eca-2179b9b5d8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResUNet_F2R(nn.Module):\n",
    "    \"\"\"\n",
    "    F2R-Backbone: Feature-Fusion-ResUNet Backbone\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 encoder='resnet50',\n",
    "                 pretrained=True,\n",
    "                 fusion_out_ch=128\n",
    "                 ):\n",
    "\n",
    "        super(ResUNet_F2R, self).__init__()\n",
    "        assert encoder in ['resnet18', 'resnet34', 'resnet50', 'resnet101', 'resnet152'], \"Incorrect encoder type\"\n",
    "        if encoder in ['resnet18', 'resnet34']:\n",
    "            filters = [64, 128, 256, 512]\n",
    "        else:\n",
    "            filters = [256, 512, 1024, 2048]\n",
    "        resnet = class_for_name(\"torchvision.models\", encoder)(pretrained=pretrained)\n",
    "\n",
    "        self.firstconv      = resnet.conv1  # H/2\n",
    "        self.firstbn        = resnet.bn1\n",
    "        self.firstrelu      = resnet.relu\n",
    "        self.firstmaxpool   = resnet.maxpool  # H/4\n",
    "\n",
    "        # Encoder\n",
    "        self.layer1 = resnet.layer1  # H/4\n",
    "        self.layer2 = resnet.layer2  # H/8\n",
    "        self.layer3 = resnet.layer3  # H/16\n",
    "\n",
    "        # Decoder\n",
    "        self.upconv3    = upconv(filters[2], 512, 3, 2)\n",
    "        self.iconv3     = conv(filters[1] + 512, 512, 3, 1)\n",
    "        self.upconv2    = upconv(512, 256, 3, 2)\n",
    "        self.iconv2     = conv(filters[0] + 256, 256, 3, 1)\n",
    "\n",
    "\t    # Feature Fusion Block\n",
    "        self.side3          = upconv_like(1024, fusion_out_ch, 3)\n",
    "        self.side2          = upconv_like(512, fusion_out_ch, 3)\n",
    "        self.side1          = conv(256, fusion_out_ch, 1, 1)\n",
    "        self.fusion_conv    = nn.Conv2d(3*fusion_out_ch,fusion_out_ch,1)\n",
    "\n",
    "        # Cross Norm. Layer\n",
    "        self.fusion_pn = PositionwiseNorm2()\n",
    "        self.fusion_cn = ChannelwiseNorm(fusion_out_ch)\n",
    "        self.fuse_weight_fusion_1 = torch.nn.Parameter(torch.FloatTensor(1), requires_grad=True)\n",
    "        self.fuse_weight_fusion_2 = torch.nn.Parameter(torch.FloatTensor(1), requires_grad=True)\n",
    "        self.fuse_weight_fusion_1.data.fill_(0.7)\n",
    "        self.fuse_weight_fusion_2.data.fill_(0.3)\n",
    "\n",
    "        self.out_channels = 192\n",
    "\n",
    "    def name(self):\n",
    "        return 'ResUNet_F2R'\n",
    "        \n",
    "    def skipconnect(self, x1, x2):\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "\n",
    "        x1 = F.pad(x1, (diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2))\n",
    "\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Encoder\n",
    "        x       = self.firstrelu(self.firstbn(self.firstconv(x)))\n",
    "        x_first = self.firstmaxpool(x)\n",
    "        \n",
    "        x1 = self.layer1(x_first)\n",
    "        x2 = self.layer2(x1)\n",
    "        x3 = self.layer3(x2)\n",
    "\n",
    "        # Decoder\n",
    "        x   = self.upconv3(x3)\n",
    "        x   = self.skipconnect(x2, x)\n",
    "        x2d = self.iconv3(x)\n",
    "\n",
    "        x = self.upconv2(x2d)\n",
    "        x = self.skipconnect(x1, x)\n",
    "        x = self.iconv2(x)\n",
    "\n",
    "        # Feature Fusion output\n",
    "        d1          = self.side1(x)\n",
    "        d2          = self.side2(x2d, d1)\n",
    "        d3          = self.side3(x3, d1)\n",
    "        x_fusion    = self.fusion_conv(torch.cat((d1,d2,d3),1))  # H/4\n",
    "        \n",
    "        del x, x2, d1, d2, d3, x2d\n",
    "\n",
    "        # Shared Coupling-bridge Normalization\n",
    "        desc1       = self.fusion_pn(x_fusion)\n",
    "        desc2       = self.fusion_cn(x_fusion)\n",
    "        x_fusion_cn = desc1 * (self.fuse_weight_fusion_1/(self.fuse_weight_fusion_1+self.fuse_weight_fusion_2)) + \\\n",
    "            desc2 * (self.fuse_weight_fusion_2/(self.fuse_weight_fusion_1+self.fuse_weight_fusion_2))\n",
    "\n",
    "        # return {\n",
    "        #     'global_map':       x3,             # Coarse level\n",
    "        #     'desc_map':         x_fusion,       # D_desc\n",
    "        #     'local_map':        x_fusion_cn,    # F_cn\n",
    "        #     'local_map_first':  x_first         # Initial feature maps\n",
    "        #     }\n",
    "\n",
    "        print(f'global map : {x3.shape}') # global feature maps\n",
    "        print(f'descriptor map : {x_fusion.shape}') # backbone 빠져나와서 cn pipeline 들어가기 전\n",
    "        print(f'local map : {x_fusion_cn.shape}') # feature of coupling-bridge normalization\n",
    "        print(f'local map first : {x_first.shape}') # initial feature maps\n",
    "        \n",
    "        return x_fusion_cn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c705c5ea-621e-4007-9403-5761088077ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global map : torch.Size([2, 1024, 14, 14])\n",
      "descriptor map : torch.Size([2, 128, 56, 56])\n",
      "local map : torch.Size([2, 128, 56, 56])\n",
      "local map first : torch.Size([2, 64, 56, 56])\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
      "              ReLU-3         [-1, 64, 112, 112]               0\n",
      "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
      "            Conv2d-5           [-1, 64, 56, 56]           4,096\n",
      "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
      "              ReLU-7           [-1, 64, 56, 56]               0\n",
      "            Conv2d-8           [-1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 56, 56]             128\n",
      "             ReLU-10           [-1, 64, 56, 56]               0\n",
      "           Conv2d-11          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-12          [-1, 256, 56, 56]             512\n",
      "           Conv2d-13          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-14          [-1, 256, 56, 56]             512\n",
      "             ReLU-15          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-16          [-1, 256, 56, 56]               0\n",
      "           Conv2d-17           [-1, 64, 56, 56]          16,384\n",
      "      BatchNorm2d-18           [-1, 64, 56, 56]             128\n",
      "             ReLU-19           [-1, 64, 56, 56]               0\n",
      "           Conv2d-20           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-21           [-1, 64, 56, 56]             128\n",
      "             ReLU-22           [-1, 64, 56, 56]               0\n",
      "           Conv2d-23          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-24          [-1, 256, 56, 56]             512\n",
      "             ReLU-25          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-26          [-1, 256, 56, 56]               0\n",
      "           Conv2d-27           [-1, 64, 56, 56]          16,384\n",
      "      BatchNorm2d-28           [-1, 64, 56, 56]             128\n",
      "             ReLU-29           [-1, 64, 56, 56]               0\n",
      "           Conv2d-30           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-31           [-1, 64, 56, 56]             128\n",
      "             ReLU-32           [-1, 64, 56, 56]               0\n",
      "           Conv2d-33          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-34          [-1, 256, 56, 56]             512\n",
      "             ReLU-35          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-36          [-1, 256, 56, 56]               0\n",
      "           Conv2d-37          [-1, 128, 56, 56]          32,768\n",
      "      BatchNorm2d-38          [-1, 128, 56, 56]             256\n",
      "             ReLU-39          [-1, 128, 56, 56]               0\n",
      "           Conv2d-40          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-41          [-1, 128, 28, 28]             256\n",
      "             ReLU-42          [-1, 128, 28, 28]               0\n",
      "           Conv2d-43          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-44          [-1, 512, 28, 28]           1,024\n",
      "           Conv2d-45          [-1, 512, 28, 28]         131,072\n",
      "      BatchNorm2d-46          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-47          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-48          [-1, 512, 28, 28]               0\n",
      "           Conv2d-49          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-50          [-1, 128, 28, 28]             256\n",
      "             ReLU-51          [-1, 128, 28, 28]               0\n",
      "           Conv2d-52          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-53          [-1, 128, 28, 28]             256\n",
      "             ReLU-54          [-1, 128, 28, 28]               0\n",
      "           Conv2d-55          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-56          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-57          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-58          [-1, 512, 28, 28]               0\n",
      "           Conv2d-59          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-60          [-1, 128, 28, 28]             256\n",
      "             ReLU-61          [-1, 128, 28, 28]               0\n",
      "           Conv2d-62          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-63          [-1, 128, 28, 28]             256\n",
      "             ReLU-64          [-1, 128, 28, 28]               0\n",
      "           Conv2d-65          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-66          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-67          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-68          [-1, 512, 28, 28]               0\n",
      "           Conv2d-69          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-70          [-1, 128, 28, 28]             256\n",
      "             ReLU-71          [-1, 128, 28, 28]               0\n",
      "           Conv2d-72          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-73          [-1, 128, 28, 28]             256\n",
      "             ReLU-74          [-1, 128, 28, 28]               0\n",
      "           Conv2d-75          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-76          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-77          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-78          [-1, 512, 28, 28]               0\n",
      "           Conv2d-79          [-1, 256, 28, 28]         131,072\n",
      "      BatchNorm2d-80          [-1, 256, 28, 28]             512\n",
      "             ReLU-81          [-1, 256, 28, 28]               0\n",
      "           Conv2d-82          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-83          [-1, 256, 14, 14]             512\n",
      "             ReLU-84          [-1, 256, 14, 14]               0\n",
      "           Conv2d-85         [-1, 1024, 14, 14]         262,144\n",
      "      BatchNorm2d-86         [-1, 1024, 14, 14]           2,048\n",
      "           Conv2d-87         [-1, 1024, 14, 14]         524,288\n",
      "      BatchNorm2d-88         [-1, 1024, 14, 14]           2,048\n",
      "             ReLU-89         [-1, 1024, 14, 14]               0\n",
      "       Bottleneck-90         [-1, 1024, 14, 14]               0\n",
      "           Conv2d-91          [-1, 256, 14, 14]         262,144\n",
      "      BatchNorm2d-92          [-1, 256, 14, 14]             512\n",
      "             ReLU-93          [-1, 256, 14, 14]               0\n",
      "           Conv2d-94          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-95          [-1, 256, 14, 14]             512\n",
      "             ReLU-96          [-1, 256, 14, 14]               0\n",
      "           Conv2d-97         [-1, 1024, 14, 14]         262,144\n",
      "      BatchNorm2d-98         [-1, 1024, 14, 14]           2,048\n",
      "             ReLU-99         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-100         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-101          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-102          [-1, 256, 14, 14]             512\n",
      "            ReLU-103          [-1, 256, 14, 14]               0\n",
      "          Conv2d-104          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-105          [-1, 256, 14, 14]             512\n",
      "            ReLU-106          [-1, 256, 14, 14]               0\n",
      "          Conv2d-107         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-108         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-109         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-110         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-111          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-112          [-1, 256, 14, 14]             512\n",
      "            ReLU-113          [-1, 256, 14, 14]               0\n",
      "          Conv2d-114          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-115          [-1, 256, 14, 14]             512\n",
      "            ReLU-116          [-1, 256, 14, 14]               0\n",
      "          Conv2d-117         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-118         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-119         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-120         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-121          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-122          [-1, 256, 14, 14]             512\n",
      "            ReLU-123          [-1, 256, 14, 14]               0\n",
      "          Conv2d-124          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-125          [-1, 256, 14, 14]             512\n",
      "            ReLU-126          [-1, 256, 14, 14]               0\n",
      "          Conv2d-127         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-128         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-129         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-130         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-131          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-132          [-1, 256, 14, 14]             512\n",
      "            ReLU-133          [-1, 256, 14, 14]               0\n",
      "          Conv2d-134          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-135          [-1, 256, 14, 14]             512\n",
      "            ReLU-136          [-1, 256, 14, 14]               0\n",
      "          Conv2d-137         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-138         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-139         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-140         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-141          [-1, 512, 28, 28]       4,719,104\n",
      "       GroupNorm-142          [-1, 512, 28, 28]           1,024\n",
      "            conv-143          [-1, 512, 28, 28]               0\n",
      "          upconv-144          [-1, 512, 28, 28]               0\n",
      "          Conv2d-145          [-1, 512, 28, 28]       4,719,104\n",
      "       GroupNorm-146          [-1, 512, 28, 28]           1,024\n",
      "            conv-147          [-1, 512, 28, 28]               0\n",
      "          Conv2d-148          [-1, 256, 56, 56]       1,179,904\n",
      "       GroupNorm-149          [-1, 256, 56, 56]             512\n",
      "            conv-150          [-1, 256, 56, 56]               0\n",
      "          upconv-151          [-1, 256, 56, 56]               0\n",
      "          Conv2d-152          [-1, 256, 56, 56]       1,179,904\n",
      "       GroupNorm-153          [-1, 256, 56, 56]             512\n",
      "            conv-154          [-1, 256, 56, 56]               0\n",
      "          Conv2d-155          [-1, 128, 56, 56]          32,896\n",
      "       GroupNorm-156          [-1, 128, 56, 56]             256\n",
      "            conv-157          [-1, 128, 56, 56]               0\n",
      "          Conv2d-158          [-1, 128, 56, 56]         589,952\n",
      "       GroupNorm-159          [-1, 128, 56, 56]             256\n",
      "            conv-160          [-1, 128, 56, 56]               0\n",
      "     upconv_like-161          [-1, 128, 56, 56]               0\n",
      "          Conv2d-162          [-1, 128, 56, 56]       1,179,776\n",
      "       GroupNorm-163          [-1, 128, 56, 56]             256\n",
      "            conv-164          [-1, 128, 56, 56]               0\n",
      "     upconv_like-165          [-1, 128, 56, 56]               0\n",
      "          Conv2d-166          [-1, 128, 56, 56]          49,280\n",
      "          Conv2d-167            [-1, 1, 56, 56]              50\n",
      "          Conv2d-168            [-1, 1, 56, 56]              50\n",
      "PositionwiseNorm2-169          [-1, 128, 56, 56]               0\n",
      " ChannelwiseNorm-170          [-1, 128, 56, 56]               0\n",
      "================================================================\n",
      "Total params: 22,197,156\n",
      "Trainable params: 22,197,156\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 377.88\n",
      "Params size (MB): 84.68\n",
      "Estimated Total Size (MB): 463.13\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "net = ResUNet_F2R().cuda()\n",
    "summary(net, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a657720e-d0c2-4fd3-8096-d72def6a690e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
