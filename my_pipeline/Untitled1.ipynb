{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ca1b468",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import shutil\n",
    "import logging\n",
    "import yaml\n",
    "import importlib\n",
    "import time\n",
    "from path import Path\n",
    "from abc import ABC, abstractmethod\n",
    "from PIL import Image as Im\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import tqdm\n",
    "\n",
    "from matplotlib.colors import ListedColormap, LinearSegmentedColormap\n",
    "from matplotlib import cm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torchsummary import summary\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import torch.distributed as dist\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "from dataloader import aachen_loader\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from feature_descriptors import backbone\n",
    "from feature_descriptors import detection_net\n",
    "# from feature_descriptors import my_model\n",
    "\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import copy\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7157dc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_partition(x, window_size):\n",
    "    # B, H, W, C : x.size -> B*Window_num, window_size, window_size, C\n",
    "    B, H, W, C = x.size() \n",
    "    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)\n",
    "    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)\n",
    "    return windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "056ae48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mlp(nn.Module):\n",
    "    def __init__(self, C, ffn_dim, act_layer=nn.GELU, drop=0.1):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(C, ffn_dim)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(ffn_dim, C)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d83d0c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinTransformerLayer(nn.Module):\n",
    "    def __init__(self, C, num_heads, window_size, ffn_dim, act_layer = nn.GELU, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        self.mlp1 = Mlp(C, ffn_dim, act_layer=nn.GELU, drop=dropout)\n",
    "        self.mlp2 = Mlp(C, ffn_dim, act_layer=nn.GELU, drop=dropout)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(C)\n",
    "        self.norm2 = nn.LayerNorm(C)\n",
    "        self.norm3 = nn.LayerNorm(C)\n",
    "        self.norm4 = nn.LayerNorm(C)\n",
    "\n",
    "\n",
    "        self.shift_size = window_size // 2\n",
    "        self.window_size = window_size\n",
    "        self.W_MSA = SwinAttention(num_heads=num_heads, C=C, dropout=dropout )\n",
    "        self.SW_MSA = SwinAttention(num_heads=num_heads, C=C, dropout=dropout )\n",
    "\n",
    "    def forward(self, x): # BS, L, C\n",
    "        BS, L, C = x.shape \n",
    "        S = int(math.sqrt(L))\n",
    "\n",
    "        shortcut = x\n",
    "\n",
    "        x = self.norm1(x) # BS, L, C\n",
    "\n",
    "        x_windows = self.window_to_attention(x, S, C)\n",
    "\n",
    "        attn_x = self.W_MSA(x_windows)\n",
    "\n",
    "        x = self.attention_to_og(attn_x, S, C)\n",
    "\n",
    "        x = x + shortcut\n",
    "\n",
    "        shorcut = x\n",
    "\n",
    "        x = self.norm2(x)\n",
    "        x = self.mlp1(x)\n",
    "\n",
    "        x = x + shortcut\n",
    "\n",
    "        shortcut = x\n",
    "\n",
    "        x = self.norm3(x)\n",
    "\n",
    "        x_windows = self.window_to_attention(x, S, C ,shift=True) # cyclic shift for SW_MSA\n",
    "\n",
    "        x_attn = self.SW_MSA(x_windows)\n",
    "\n",
    "        x = self.attention_to_og(x, S, C ,shift=True) # reverse cyclic shift for SW_MSA\n",
    "\n",
    "        x = x+ shortcut\n",
    "\n",
    "        shortcut = x\n",
    "\n",
    "        x = self.norm4(x)\n",
    "        x = self.mlp2(x)\n",
    "\n",
    "        return x + shortcut\n",
    "\n",
    "    def window_to_attention(self, x, S, C, shift = False):\n",
    "        x = x.view(-1, S, S, C)\n",
    "        if shift :\n",
    "            x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))\n",
    "        x_windows = window_partition(x, self.window_size)\n",
    "        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)\n",
    "        return x_windows\n",
    "\n",
    "    def attention_to_og(self, attn_x, S, C,shift=False):\n",
    "        attn_x = attn_x.view(-1, self.window_size, self.window_size, C)\n",
    "        x = window_reverse(attn_x, self.window_size, S, S)\n",
    "        if shift :\n",
    "            x = torch.roll(x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))\n",
    "        x = x.view(-1, S*S, C)\n",
    "        return x\n",
    "class SwinAttention(nn.Module):\n",
    "    def __init__(self, num_heads, C, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.scale = C ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(C, C * 3, bias=True)\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "        self.attn_drop = nn.Dropout(0.1)\n",
    "\n",
    "        self.proj = nn.Linear(C, C)\n",
    "        self.proj_drop = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self ,x):# BS, L, C\n",
    "        # x = [B, H, W, C]\n",
    "        B, L, C = x.shape\n",
    "\n",
    "\n",
    "        qkv = self.qkv(x).reshape(B, L, 3, self.num_heads, C//self.num_heads).permute(2,0,3,1,4) # 3, B, Head, L, C_v\n",
    "\n",
    "        q, k, v= qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        q = q*self.scale\n",
    "\n",
    "        attn = (q @ k.transpose(-1,-2)) # dot product\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        여기서부터 attention 작업\n",
    "        \"\"\"\n",
    "\n",
    "        attn_score = self.softmax(attn)\n",
    "        attn_score = self.attn_drop(attn_score) # L, L\n",
    "        # B, Head, L, C_v\n",
    "\n",
    "        out = (attn @ v).transpose(1,2).flatten(-2) # B, L, C \n",
    "\n",
    "\n",
    "        out = self.proj(out)\n",
    "        out = self.proj_drop(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9bdf4ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = SwinTransformerLayer(C=32, num_heads=16, window_size=224, ffn_dim=5120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9c57c318",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/minseong/.pyenv/versions/vl-pipeline/lib/python3.8/site-packages/torch/cuda/__init__.py:146: UserWarning: \n",
      "NVIDIA GeForce RTX 3090 with CUDA capability sm_86 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_70.\n",
      "If you want to use the NVIDIA GeForce RTX 3090 GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(incompatible_device_warn.format(device_name, capability, \" \".join(arch_list), device_name))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msummary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m224\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m224\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/vl-pipeline/lib/python3.8/site-packages/torchsummary/torchsummary.py:72\u001b[0m, in \u001b[0;36msummary\u001b[0;34m(model, input_size, batch_size, device)\u001b[0m\n\u001b[1;32m     68\u001b[0m model\u001b[38;5;241m.\u001b[39mapply(register_hook)\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# make a forward pass\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# print(x.shape)\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# remove these hooks\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m hooks:\n",
      "File \u001b[0;32m~/.pyenv/versions/vl-pipeline/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[17], line 19\u001b[0m, in \u001b[0;36mSwinTransformerLayer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x): \u001b[38;5;66;03m# BS, L, C\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m     BS, L, C \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape \n\u001b[1;32m     20\u001b[0m     S \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(math\u001b[38;5;241m.\u001b[39msqrt(L))\n\u001b[1;32m     22\u001b[0m     shortcut \u001b[38;5;241m=\u001b[39m x\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 3)"
     ]
    }
   ],
   "source": [
    "summary(net, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc86a4ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VisLoc",
   "language": "python",
   "name": "vl-pipeline"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
